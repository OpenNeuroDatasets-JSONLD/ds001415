The Audio Cartography project investigated the influence of temporal arrangement
on the interpretation of information from a simple spatial data set.  Three 
auditory map types (audio types) were designed and implemented, and differences 
in the responses between those audio types were evaluated.

Simplified raster data (eight rows x eight columns) was represented in three types 
of audio. First, a "sequential" representation read values one at a time from each 
cell of the raster, following an English reading order, and encoded the data value 
as loudness of a single fixed-duration and fixed-frequency note.  Second, an 
augmented-sequential ("augmented") representation used the same reading order, but 
encoded the data value as volume, the row as frequency, and the column as the rate 
of the notes play (constant total cell duration).  Third, a "concurrent" 
representation used the same encoding as the augmented type, but allowed the notes
to overlap in time.  

Participants completed a training session in a computer-lab setting, where they
were introduced to the audio types and practiced making a comparison between  
data values at two locations within the display based on what they heard.  The 
training sessions, including associated paperwork, lasted up to one hour.  In a 
second study session, participants listened to the auditory maps and made 
decisions about the data they represented while the fMRI scanner recorded 
digital brain images.

The behavioral task consisted of listening to an auditory representation of 
geospatial data ("map"), and then making a decision about the relative values 
of data at two specified locations.  After listening to the map ("listen"), a 
graphic depicted two locations within a square (white background).  Each 
location was marked with a small square (size: 2x2 grid cells); one square was 
symbolized with a black solid outline and transparent black fill, the other was 
symbolized with a red dashed outline and transparent red fill.  The decision 
("response") was made under one of two balanced conditions.  Under the active 
listening condition ("active") the map was played a second time while 
participants made their decision; in the memory condition ("memory"), 
a decision was made in relative quiet (the scanner noises and intermittent 
acquisition noise persisted).  During the initial map listening, participants 
were aware of neither the locations of the response options within the map 
extent, nor the response conditions under which they would make their 
decision.  Participants could response any time after the graphic was 
displayed; once a response was entered, the playback stopped (active response 
condition only) and the presentation continued to the next trial.

Additional details about the specific maps used in this are available from 
[REPLACE ScholarsBank DOI]...
Listed response options met the criteria that:
- the four continuous block (2x2) of grid cells shared the same data value, 
- one option had data value "high" and the other had data value "low",
- the two response options were offset both horizontally and vertically,
- the black option was to the west (left) of the red option.

Data was collected in accordance with a protocol approved by the Institutional Review Board at the University of Oregon.

Details of the design process and evaluation are provided in 
[REPLACE dissertation DOI].

Scripts that created the experimental stimuli and automated the execution of
the processing steps described below are available from [REPLACE ScholarsBank URL].


Processing Applied to fMRI Images
==================================
Conversion of the DICOM files produced by the scanner to NiFTi format was 
performed by MRIConvert (LCNI).  Orientation to standard axes was performed 
and recorded in the NiFTi header (FMRIB, fslreorient2std).  The excess slices 
in the anatomical images that represented tissue in the next were trimmed 
(FMRIB, robustfov).  Participant identity was protected through automated 
defacing of the anatomical data (FreeSurfer, mri_deface), with additional 
post-processing to ensure that no brain voxels were erroneously removed from 
the image (FMRIB, BET; brain mask dilated with three iterations 
"fslmaths -dilM"). 

Neuroimaging data is sorted by participant and image type:
	"sub-NN/anat"
	"sub-NN/fmap"
	"sub-NN/func"


Preparation of Metadata
========================
The dcm2niix tool (Rorden) was used to create draft JSON sidecar files 
with metadata extracted from the DICOM headers. The draft sidecar file were 
revised to augment the JSON elements with additional tags (e.g., "Orientation"
and "TaskDescription") and to make a more human-friendly version of tag 
contents (e.g., "InstitutionAddress" and "DepartmentName").  The device serial 
number was constant throughout the data collection (i.e., all data collection 
was conducted on the same scanner), and the respective metadata values were 
replaced with an anonymous identifier: "Scanner1".  

Metadata are co-located with the files that they describe.


Extraction and Formatting of Behavioral Data
============================================
Stimuli were presented using PsychoPy (Peirce, 2007), which produced log files
from which event details were extracted.  

Log files are available in "sourcedata/behavioral".
Extracted event details accompany BOLD images in "sub-NN/func/*events.tsv".
Three column explanatory variable files are in "derivatives/ev/sub-NN".


References
===========
Audacity

FMRIB (Functional Magnetic Resonance Imaging of the Brain). FMRIB Software 
	Library (FSL; fslreorient2std, robustfov, BET).  Oxford, v5.0.9, Available:
	https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/
	
FreeSurfer (mri_deface). Harvard, v1.22, Available: 
	https://surfer.nmr.mgh.harvard.edu/fswiki/AutomatedDefacingTools)

LCNI (Lewis Center for Neuroimaging). MRIConvert (mcverter), v2.1.0 build 440, 
	Available: 
	https://lcni.uoregon.edu/downloads/mriconvert/mriconvert-and-mcverter

Peirce, JW. PsychoPy–psychophysics software in Python. Journal of Neuroscience 
	Methods, 162(1–2):8 – 13, 2007. Software Available: http://www.psychopy.org/

Python

Pyo

R

Rorden, C. dcm2niix, v1.0.20171215, Available: 
	https://github.com/rordenlab/dcm2niix

